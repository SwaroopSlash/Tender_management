{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from typing import List, Dict\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "UPLOADED_PDFs_DIR = \"./uploaded_pdfs_1\"\n",
    "FAISS_INDEX_DIR = \"./faiss_indexes\"  # Directory to store individual FAISS indexes\n",
    "PDF_METADATA_FILE = \"./pdf_metadata.json\"  # File to store PDF metadata\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "class PDFQuestionAnswering:\n",
    "    def __init__(self, google_api_key: str):\n",
    "        self.google_api_key = google_api_key\n",
    "        genai.configure(api_key=self.google_api_key)\n",
    "        \n",
    "        # Initialize embeddings and LLM\n",
    "        self.embed_model = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=self.google_api_key\n",
    "        )\n",
    "\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            google_api_key=self.google_api_key,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        # Create necessary directories\n",
    "        os.makedirs(UPLOADED_PDFs_DIR, exist_ok=True)\n",
    "        os.makedirs(FAISS_INDEX_DIR, exist_ok=True)\n",
    "        \n",
    "        # Load or initialize PDF metadata\n",
    "        self.pdf_metadata = self.load_pdf_metadata()\n",
    "        \n",
    "        # Initialize vector stores for each PDF\n",
    "        self.vector_stores = {}\n",
    "        self.load_all_vector_stores()\n",
    "    \n",
    "    def load_pdf_metadata(self):\n",
    "        \"\"\"Load PDF metadata from file or create new if doesn't exist\"\"\"\n",
    "        if os.path.exists(PDF_METADATA_FILE):\n",
    "            with open(PDF_METADATA_FILE, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def save_pdf_metadata(self):\n",
    "        \"\"\"Save PDF metadata to file\"\"\"\n",
    "        with open(PDF_METADATA_FILE, 'w') as f:\n",
    "            json.dump(self.pdf_metadata, f)\n",
    "    \n",
    "    def get_index_path(self, pdf_name: str):\n",
    "        \"\"\"Get the path for a PDF's FAISS index\"\"\"\n",
    "        return os.path.join(FAISS_INDEX_DIR, pdf_name.replace('.pdf', ''))\n",
    "    \n",
    "    def load_all_vector_stores(self):\n",
    "        \"\"\"Load all existing vector stores\"\"\"\n",
    "        for pdf_name in self.pdf_metadata:\n",
    "            index_path = self.get_index_path(pdf_name)\n",
    "            if os.path.exists(index_path):\n",
    "                try:\n",
    "                    self.vector_stores[pdf_name] = FAISS.load_local(\n",
    "                        index_path,\n",
    "                        self.embed_model,\n",
    "                        allow_dangerous_deserialization = True\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error loading index for {pdf_name}: {str(e)}\")\n",
    "\n",
    "    def process_pdf(self, pdf_path: str, pdf_name: str):\n",
    "        \"\"\"Process a PDF file and store its embeddings\"\"\"\n",
    "        try:\n",
    "            # Check if PDF has already been processed\n",
    "            if pdf_name in self.pdf_metadata:\n",
    "                print(f\"{pdf_name} is already processed and available in metadata\")\n",
    "                return True\n",
    "\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            for i, doc in enumerate(documents):\n",
    "                doc.metadata[\"page\"] = i + 1\n",
    "                doc.metadata[\"pdf_name\"] = pdf_name\n",
    "            \n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP\n",
    "            )\n",
    "            splits = text_splitter.split_documents(documents)\n",
    "            \n",
    "            # Create and save vector store for this PDF\n",
    "            vector_store = FAISS.from_documents(splits, self.embed_model)\n",
    "            index_path = self.get_index_path(pdf_name)\n",
    "            vector_store.save_local(index_path)\n",
    "            \n",
    "            # Update metadata and save\n",
    "            self.vector_stores[pdf_name] = vector_store\n",
    "            self.pdf_metadata[pdf_name] = {\n",
    "                \"pages\": len(documents),\n",
    "                \"chunks\": len(splits),\n",
    "                \"date_added\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            self.save_pdf_metadata()\n",
    "\n",
    "            print('Stored successfully..!!')\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in processing PDF: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_answer(self, question: str, pdf_name: str = None) -> Dict:\n",
    "        \"\"\"Get answer for a question with source context\"\"\"\n",
    "        try:\n",
    "            if pdf_name and pdf_name not in self.vector_stores:\n",
    "                raise ValueError(f\"PDF {pdf_name} not found in processed documents\")\n",
    "            \n",
    "            vector_store = self.vector_stores[pdf_name]\n",
    "            \n",
    "            qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=self.llm,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                return_source_documents=True\n",
    "            )\n",
    "            \n",
    "            result = qa_chain({\"query\": question})\n",
    "            \n",
    "            sources = []\n",
    "            for doc in result[\"source_documents\"]:\n",
    "                sources.append({\n",
    "                    \"page\": doc.metadata[\"page\"],\n",
    "                    \"pdf_name\": doc.metadata[\"pdf_name\"],\n",
    "                    \"content\": doc.page_content\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": result[\"result\"],\n",
    "                \"sources\": sources\n",
    "            }\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error in getting answer: {str(e)}\")\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": f\"Error: {str(e)}\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "    \n",
    "    def delete_pdf(self, pdf_name: str):\n",
    "        \"\"\"Delete a PDF and its associated data\"\"\"\n",
    "        try:\n",
    "            # Delete FAISS index\n",
    "            index_path = self.get_index_path(pdf_name)\n",
    "            if os.path.exists(index_path):\n",
    "                shutil.rmtree(index_path)\n",
    "            \n",
    "            # Delete from vector stores dict\n",
    "            if pdf_name in self.vector_stores:\n",
    "                del self.vector_stores[pdf_name]\n",
    "            \n",
    "            # Delete from metadata\n",
    "            if pdf_name in self.pdf_metadata:\n",
    "                del self.pdf_metadata[pdf_name]\n",
    "                self.save_pdf_metadata()\n",
    "            \n",
    "            # Delete PDF file\n",
    "            pdf_path = os.path.join(UPLOADED_PDFs_DIR, pdf_name)\n",
    "            if os.path.exists(pdf_path):\n",
    "                os.remove(pdf_path)\n",
    "                \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error in deleting PDF: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_processed_pdfs(self):\n",
    "        \"\"\"Get list of processed PDFs with metadata\"\"\"\n",
    "        return self.pdf_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"AIzaSyB9VMHO-m_fkL318QDQuL5NRW2JQL8m0lw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = PDFQuestionAnswering(GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_name = 'report (3).pdf'\n",
    "pdf_save_path = os.path.join(UPLOADED_PDFs_DIR, pdf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report (3).pdf is already processed and available in metadata\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.process_pdf(pdf_name= pdf_name, pdf_path= pdf_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenderGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
